پس نیجه شد که مشترک باشند و مشترکها اول و تخصصی ها وسط باشند نتایج خوبی حاصل خواهد شد.

آزمایش: متفاوت بزار او ببین چی می شه و مقایسه کن

جالبه اگر روی یک دیتاست ترین بشه جوابها می ره برای بیرون کشیدن همون رابطه ... مثلا برای همین رابطه ایکس اتر... اما اگر مالتی بشه و در هم بشه ... پر کردن به شکل طبیعی که در متن میاد انگار مهمتر می شه... حالا نمی دونم این خوب هست یا نه... آخه چون یکجا بحث آور فیتینگ هم بود... یه جاهایی باید به همون داده های اصلی و یا به نوعی استخراج روابط هم اتکا کرد.

کامـ مید ظاهرا از کام پری بهتر است.

نرخ یادگیری با تعداد نمونه رابطه داره... وقتی نمونه کم است نرخ یادگیری باید بالاتر باشه.

به نظر مید که اجازه بدیم خود پرامپت شکل بگیره نتایج خوبی رو ایجاد می کنه.
روشها به نرخ یادگیری و روش حتی ساپ و آساپ ربط داره... آنساپ در تعداد کم و شاید نرخ یادگیری کم بهتر کار می کنه... اما ساپ برای نرخ یادگیری بالاتر و نمونه های بیشتر و غیره کاربرد داره... این رو دقت کن
نرخ یادگیری کم برای نمونه زیاد مناسبه نه برعکس
و مخصوصا که تعداد توکن ها هم زیاد باشه
ایپاک زیاد هم وقتی داده کم است می تونه موجب اورفیتینگ بشه.
توی موارد کم مقداردهی از کلمات می تونه باز موثر باشه مخصوصا اگر تعداد توکنها زیاد باشه
مورد جالب
با تعداد توکن بالا ولی استفاده از mlp که شبکه سبکتری است و با نرخ کم و مقداردهی اولیه تصادفی و ایپاک بالا نتیجه خوبی حاصل شده انگار خوب اپتیمایز شده
برای lstm می تونی توکن بالا استفاده کنی اما نرخ یادگیری رو ببری بالا، البته mlp هم با مقدار کلماتی خوب کار کرده... البته با توکن پایین خوب کار نمی کنه...
به نظر lstm روی داده بیشتر هم زود اورفیت می شه اما این برای mlp کمتره 
کلا تعداد ایپاک و تعداد توکن و نرخ یادگیری خیلی مهم هستند. نرخ یادگیری کم و ایپاک کم غیر از اورفیتینگ اصلا انگار چیزی یاد نمی گیره... یعنی یادگیری در ایپاک بالاتر و نرخ یادگیری مناسب با تعداد توکن هاست... دلیل تکرار همون ایپاک کم است.

انواع روشهای توجه رو بررسی کردم. برای تسکهای مختلف ممکنه توجه متفاوت باشه... مثلا در تسک xAttr به ورودی شاید بیشتر توجه بشه تا تسکهای دیگه

اون توکنهای ثابتی که به تسک ها اضافه می کنیم هم تاثیر داره.... یعنی توجه به اونها زیاد میشه برای یادگیری

بدون اضافه کردن تارگت به نظر linear می تونه حتی با نگاه به ورودی و تارگت نتایج بدی نداشته باشه.
بدون توجه به هیچ چیز توجه به خود یا target در rb می شه مثل همون حالت معمول احتمالا ... 
کلا اگر به هیچ چیز توجه نشه و target هم اضافه نشه چیزی یادگیری نمیشه
از طرفی اگر تارگت اضافه نشه و یا تارگت بهش توجه نشه و به هرچیزی هم توجه بشه چیزی یادگیری نمی شه یعنی باز تاثیر خود تارگت هم زیاد است. حداقل توی rb اینطوریه
به نظرم rb چیزی به اون صورت یاد نمی گیره و کلا به همون یکی از اضافه کردن یا توجه به تارگت کار می کنه و کلا احتمالات رو مساوی تقسیم می کنه (نه! rb با نرخ یادگیری پایین کار کرد!)
توجه و افزودن تارگت مثل هم هستند ولی به نظر توجه به تارگت بهتر باشه و یا بشه این رو استفاده کرد.
در مورد sub و ایکس اینتنت توجه و اضافه کردن تارگت یک نتیجه داده یعنی شاید توجه احتمال یک می ده به ... پس فقط در صورتی که توجه به دوتاش بشه اونوقت نرخ یادگیری تاثیر داره و نرخ یادگیری پایینتر بهتره ... ولی خوب توجه به دوتا چه فایده ای داره یعنی بعد از اینکه توجه کرد با خودش جمع می شه
*** نرخ یادگیری پایینتر برای پرامپت لرنینگ نتایج بهتری میده
در linear توجه و افزودن با هم نتایج خوبی به همراه داره البته شاید باز به خاطر افزودن باشه یعنی جایی که افزوده شده نتیجه بهتر شده نمی دونم چون در حالت دیگه نتیجه مساوی است فرقش اینجا اینه که انگار تارگت با خودش جمع می شه یعنی دو تارگت میشه ... شاید بحث های نرمالسازی هم به کار بیاد.
به نظر توجه به ورودی هم بهتر از توجه صرف به تارگت نیست. 
** به هر سه تاشون توجه بشه باز نتایج خوب نیست. (یعنی به سورس تارگت و ورودی) البته در Sub مشاهده شد. 
** اگر به تارگت توجه نشه توجه به سورسها چیزهایی رو پیشبینی می کنه (اشتباه شد). بدترین حالت اینه که فقط توجه به ورودی باشه.
!!! توجه به تارگت موقعی که اون دو تا فعال باشند فایده ای نداره!!!!!
در rb به نظر توجه به سورس بد نیست.
توی rb مشکل از temperature بود.
اما در روزش ترکیب (compose) اگر از خود همون موارد باشه به نظر اضافه کردن یا نکردن تارگت تاثیری نداره. sub و rb هم مثل هم عمل کردن ولی از لحاظ توجه در sub توجه روی توکن اول بیشتره در rb روی توکن وسط

توی rb توکنهای بیشتر و temperature کمتر بهتر تفکیک می کنه و بهتر است و همینطور unsup 
اگر توجه ها شبیه هم بشن نتایج شبیه میشه مثل نتایج xIntent و xWant باید به دلیلی تفکیک بشن (شاید prefix و همینطور temperature

استفاده از lstm فقط بیشتر تکرار می کنه جمله رو ولی mlp جوابها به پیشبینی و یادگیری نزدیکتر است. جوابهای emb هم ناکامل و با کیفیت کمتری از mlp است و به نظر mlp از همه بهتر است.
به نظر میاد انتخاب تاپ ها مثلا ۳ یا ۵ از مهارتهای زیاد (سورس انکودرها) می تونه موثر باشه. 
با تمپریچر پایین wavg هم بد کار نمی کنه هرچند در کل همون cat  بهتر میاد.
توی این روشها جدید یا همون rb ظاهر target رو اضافه نکنیم بهتر می شه جواب ها البته از لحاظ تنوع...اما از لحاظ دقت اسمی به نظر اضافه کردن و توجهات تاثیراتی داره.
از روش annealing هم برای temperature استفاده کردیم.
البته سوال اصلی در این مساله اینه که چه پترنی برای یادگیری وجود داره؟ آیا حفظ کردن متون بهتره و یا یک فهم ابتدایی از پترنها
توجه به ورودی انگار باعث تنوع بیشتر و یا تکراری های کمتر می شه
با همون rb و استفاده از چیزی مثل sigmoid دیگه نیازی به توجه به تارگت هم نیست که برای هر رابطه تخصصی بشه و جواب می ده حتی با rb هم جواب میده اگر مثالها کافی باشه و تنوع زیادی هم به همراه داره
جالبه اینجا annealing مثبت یعنی افزایش دما (ولی خوب ثابت شده احتمالات) جواب می ده
با تعداد ۸ و دوباره انتخاب ۸ و همون anneal-rate=-1 جوابهای خوبی می ده
توی این روش جدید اضافه کردن add-target کلا خراب کرده و این جالب است که ما حاصل بقیه خوب کار می کنه
در این حالت attn-target هم انگار تاثیر زیادی نداره و منفیش بیشتر از مثبتش است.
شرایط برای خوب بودن xAttr
یا temperature بالا باشه که شاید تنوع کم بشه و احتمالات نزدیک به هم باشند. البته تنوع از بقیه موارد در این حالت بیشتر بود!!
یا توجه به target بشه
یا anealing مثبت باشه و باز بزرگ بشن که می شه مورد اول
یا هم اون مورد ۱۵ و ۱۵ با انیلینگ منفی که به نظر میاد از بقیه بهتر باشه یعنی داره از همه طول پرامپت استفاده می کنه
** توی برخی موارد حتی اضافه کردن نمونه ها باعث تنوع کم شده که حتی دقت خوبی هم ندارند.
خرابها (با دقت زیر ۵)
اونهایی که anealing صعودی بوده و احتمالا نرخ هم بالا بوده و زیادی رفتن بالا و احتمالات یکی شده
یک سری با دمای پایین هم داریم که بد بودن - بعضی هاشون هم متوسط بودن یعنی انگار نوسان داره 
ببخشید خیلی هاشون تستها بودن که تعداد ترین کم بوده
طول پرامپت کم باشه روش wavg بود باشه تعداد سورسها زیاد بوده (مثلا ۲۰ تا) به دلیل احتمالات نه خیلی پایین و نه خیلی بالا قاطی شدند انگار
بعضیها شون هم نمی دونم چرا متفاوت عمل کردند!!!! 

در مقایسه ۸-۱ و ۸-۸ برای cola و mnli مورد 8-8 برای mnli بهتر عمل کرده و rb هم خوب بوده... جالبیش اینه که قاطی هم نکرده.
توجه به تارگت در بعضی موارد موجب بیش برازش می شه و یا کلا باعث تکرار پیشبینی و یا افتادن در قله محلی میشه
*** توجه به ورودی بستگی داره که تسکها چی باشند!!! اگر تسکها ورودی مشابه داشته باشند انگار این داره خرابش می کنه... ولی اگر تسکها متفاوت باشند یه جورهایی انگار لازم است.... این هم نکته مهمی است به نظرم

** خود استفاده از sup یا unsup خیلی مهم است یعنی برای xAttr استفاده از unsup است که جواب می ده

** یک نکته دیگه اینه که وقتی num_src و num_target مساوی باشند بهتر انتخابها بر اساس صفر و یک انجام می شه اما وقتی num_target خیلی کمتر باشه خیلی پخش می شه البته در این حالت فقط تنوع زیاد می شه اما این تونع همپوشانی تسکهای دیگه و بعضا غیر مرتبط است (دقت کن که ورودی یکسان هم ممکن است داشته باشند)


***تعداد توکنها هر چه بیشتر باشد بهتر است و جملات طبیعی تری تولید می شود.
**روش مهم دیگه توجه به تارگت یا شریک کردن تارگت است (attn_target یا target_share) در این حالت نقش برنولی ایجاد تنوع می شه حتی برای یک ورودی
** اونهایی که نمونه های کمتری دارند کلا به نفعشون است این عمل
روش sigmoid چون اطلاعات کمتری از دست می ده بهتر از sign است. اما شاید بضعی چیزها را باید دور انداخت.
جالبیش اینه که در rb می تونه دنباله های متنوعی تولید کنه... این تنوع از کجا میاد!!‌ یعنی توی generation است نه بستگی به توجه داره ... سوال این است که به چی توجه می شه که متنوعشون می کنه؟ پس شاید اگر تعدادشون زیاد بشه مجبور پخش تر بشه و بهتر بشه؟
ببین به نحوه یادگیری و بهینه سازی دقت کن... این تکه ها باید با یک ترتیب خاص در تارگت قرار بگیرند و یک جمله با معنی تولید بشه... خوب الان چه ترتیبی؟ یک سریها رو بزاریم اختصاصی باشند با چی؟ با همون مسک دیگه نیازی به تارگت هم نیست چون تارگت یعنی کلش رو جمع کنی که معنی خاصی نمی ده... 
** آزمایش ابلیشن... private ها تاثیر داشتند...
بین add_target , attend_target و حالت private باید ببینی کدوم موثر است و یا ترکیبشون... به نظر   attend_target با امکان یادگیری target_share خوبه!!! اما مگر target_share به غیر از add_target هست؟
گزینه add_target هم خوبه اما حداقل در حالت تنها private وضعیت خیلی خراب است بدون سورسها
بعضی روشها و تنظیمات یکسان کار می کنند اما دلیل اختلاف نتایج همون رندوم بودن قسمتهای رندوم است که مثلا شامل اون قسمت بشه یا نشه مثل تنظیم target_share که نیاز به رندوم داره وگرنه نتایج یکسان می شن
به نظر attn_target نباید خیلی موثر باشه نقشش در واقع شبیه یکی از همون سورسهاست و یا یک انکدری که یک کانولوشن داشته باشه... اما add_target شاید موثرتر باشه چون با کل طول تارگت جمع می شه و شاید حتی یک ارتباط بین اینها ایجاد کنه

در مقایسه wavg و cat به نظر در اولی attend_input تاثیر مثبت داره و در دومی تاثیری نداره و یا تاثیر منفی داره... البته ظاهرا در هر دو به تسک مربوط می شه...

در طبقه بندی ارتباطات نقش after زیاد بود... بعد نقش target_share قابل ملاحظه بود. نقش asc خوب بود و نقش ad (جهت انیلینگ).
طبق انتظار متوسط گیری با before یا after تفاوتی نداره ولی cat با after جواب بهتری میده. البته target_share برابر با 1 هم خوب کار کرده و در واقع این مبنای مقایسه است.

موقع cat کردن و بدون add_target به نظر انیلینگ روی -۱ تاثیر خوبی داره. بعد برای -۱ دوباره after برای soft_prompts بهتر عمل کرده (نمی شه گفت نتایج پراکنده است)
ببین وقتی مثالها کم هستند خود مثالها و تنوع اونها خیلی تاثیر داره که آیا باعث بیش برازش می شه یا نمی شه و چجوری جلوش رو بگیریم.

اگر const استفاده بشه و softmax گرفته بشه نتایج بد نیست ولی اگر normalize بشه خوب نیست شاید سافت مکس و یا normsoftmax یا همون normafter خودش باعث آموزش بهتر میشه... از طرفی خوب softmax برای اعداد منفی و مثبت درست کار می کنه اما normalize بیشتر برای اعداد مثبت خوبه.... 

ولی نکته این بود که انگار داره متوسط گیری می کنه... فقط توی حالت direct و rb و nothing هست که شاید آموزش و احتمالات متفاوت خودش رو نشون بده که باید آزمایش بشه....

در PT به نظر یک پرامپت مشترک بین تسکها حتی از اشتراکات پاره ای بهتر کار می کنه و هر دو از بدون اشتراک بهترن
طبق نمودارها به نظر میاد انتخاب همه پرامپتهای منبع از انتخاب یکی دو تا هم در حالت wavg و هم در حالت cat بهتر باشه از این نظر که سهم تسکها بهتر مشخص می شه !!!!! البته از لحاظ بازدهی انتخاب یکی دو تا در حالت CAT به نظر بهتر جواب می ده و منطقی هم هست. مخصوصا زمانی که تسکهای مرتبط توی منابع باشه اگر نباشه انتخاب همه به نظر بهتر جواب میده

برای sel_positives ظاهرا anneal_dir تاثیر زیادی داره و اگر -۱ باشه جوابها خراب می شه ولی اگر ۰ باشه نه؟! چرا؟ شاید توی حالت اول منفی ها بیشتر می شن؟ anneal_dir چه تاثیری روی مقادیر روتر داره؟ می دونم که باید مقادیر احتمال نزدیبک به صفر و یک تولید کنه. البته -1 زمانی که sel_positives نداشته باشیم کلا بهتر کار می کنه

نکته عجیب در learn_loaded_prompt این هست که اگر True باشه و نرخ یادگیری کم باشه بازدهی میاد پایین ولی اگر False باشه نرخ یادگیری تاثیر کمتری داره و البته بالاتر باشه بهتر است.

آزمایش دیگه نشون می ده که ادمه دادن ترین با همون source_prompt لود شده نتایج خوبی رو (حداقل در تک وظیفه‌ای) داره.
شاید ترین کردن بقیه پرامپتها نیاز به داده داره و یا پرامپت قبلی رو مختل می کنه. در حالت مالتی بسنجش


به نظر وقتی که multi باشه اتفاقا در مقایسه با sep هر تسکی روی تسکهای سورس مشابه هدایت می شه و این بهتر هست.

به نظر multi به تنهایی بدون add target و یا روشهای دیگه حتی نسبت به PT موثر نیست... اگر بخواد چیزی موثرش کنه استفاده از پرامپتهای بدست آمده و بعدش فاینتیون کردن روی تسکهاست. فاینتیون که چی بگم یعنی همون add-target و غیره
همون sru و البته با base هاش باید مقایسه بشه


انگار توی multi وقتی که سورسهای زیادی استفاده می شه خوب پارامترها بیشتر می شه و این کل پارامترها هستند که ظرفیت یادگیری توام رو بیشتر می کنند و انگار به نتایج بهتر می رسه.... وقتی مجبورش کردم که روی تسکهای جداگانه متمرکز بشه یا اورفیت شد و نتایجش حتی به همون pt هم نرسید!!! زمانی که توی روتر به شکل دستی احتمالها را زیاد کنیم.

انگار در multi نیسبت به single حتی شاید بشه گفت که پرامپتها تخصصی تر می شن... چون تسکهای دیگه می زنند توی سر یک تسک و رقابت دارند با هم... در حالت single انگار از همه سورسها استفاده می شه و دخالت دارند و هموارتر می شه.

یک نکته دیگه ظاهرا وقتی ما private prompt استفاده می کنیم و بعد مدل هم بره سروقت اونها و چون اونها مشترک نیستند خوب لطفشون به بقیه نمی رسه!!!

نکته این شد که تخصصی ها نقش جبران سازی را در SIP و بقیه دارند. البته اگر تسکها به هم بخورند بدون تخصصی هم بازدهی می ره بالا یعنی همون SIL اما در غیر اینصورت تخصصی ها به کمک میان.... در حالت SIP حتی اگر بخورند باز از P استفاده می شه که نقش جبران ساز یا تبدیل کننده برچسب ها به هم رو بازی کنه...

کلا قضیه سر برچسبهای مشترک است و آیا اینکه تحت عواملی می شه که برچسبهای غیرمشترک مشترک بشن....

در حالت SILPI امکان تبادل موثر بهتر وجود دارد.

به جهت هم دقت کن... یعنی مثلا NLI ها توجه خاصی روی STSB دارند. توی شباهتها مثلا cola به خاطر مشکلش اومده از mrpc استفاده کرده و یک شباهتی نشون داده می شه اما mrpc نیازی به cola نداشته و توجه ای هم روش نداشته پس در این موارد باید جهت رو بشه مشخص کرد.

توی نمونه های زیاد ممکنه بعضی شباهتها مشخص نشه چون هر کسی بار خودش رو بسته و دیگه نیازی نداره... از طرفی توی تعداد کم شاید نتونند کیس مناسب رو پیدا کنند.

در این حالتهایی که اینیت می کنی باز هم احتمال داره که یک سورس پرامپت کلا فانکشنش عوض بشه و بره به سمت کار دیگری و اون کسی که به این وابسته بود بره سر وقت یک پرامپت دیگری... شاید در کل شباهتها غیر مستقیم حفظ بشه... یعنی سورسها جابجا بشن
